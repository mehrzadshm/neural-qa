{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b16c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mehrzad/anaconda3/envs/dbpedia/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd596400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'\\nDevice: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7f601",
   "metadata": {},
   "source": [
    "#\n",
    "##  starcoderbase-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## needs HF token with accepted terms of use for this repo\n",
    "# !huggingface-cli logout\n",
    "# !huggingface-cli login\n",
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c9f860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.json',\n",
       " 'generation_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'model.safetensors',\n",
       " 'merges.txt',\n",
       " 'config.json',\n",
       " 'vocab.json',\n",
       " 'special_tokens_map.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/home/mehrzad/.cache/huggingface/hub/models--bigcode--starcoderbase-1b/snapshots/109288100c382424b748a488287e4ef7cd7007b3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefc4676",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, use_auth_token=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9eab7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2048)\n",
       "    (wpe): Embedding(8192, 2048)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4911fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for;\n",
      "# what is the population of Italy?\n",
      "# use following prefixes:\n",
      "\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "\n",
      "SELECT?country?population\n",
      "WHERE {\n",
      "?country dbr:population?population.\n",
      "?country dbo:countryName \"Italy\".\n",
      "}\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for;\n",
    "# what is the population of Italy?\n",
    "# use following prefixes:\n",
    "\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "111def1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for:\n",
      "# What was the population of Italy in year 2000?\n",
      "\n",
      "# use following prefixes:\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX owl: <http://www.w3.org/2002/07\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for:\n",
    "# What was the population of Italy in year 2000 ?\n",
    "\n",
    "# use following prefixes:\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93e8a7",
   "metadata": {},
   "source": [
    "#\n",
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca733eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3382376</td>\n",
       "      <td># write a SPARQL query for:\\n# How many ICD9 d...</td>\n",
       "      <td>select count(*) as ?x where{dbr:Friedreich's_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2559914</td>\n",
       "      <td># write a SPARQL query for:\\n# What was the fa...</td>\n",
       "      <td>select ?x where{dbr:Destrehan_High_School dbo:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2990831</td>\n",
       "      <td># write a SPARQL query for:\\n# Does adventist ...</td>\n",
       "      <td>ask where{dbr:Adventist_Girls_High_School dbo:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1637967</td>\n",
       "      <td># write a SPARQL query for:\\n# Did aliens area...</td>\n",
       "      <td>ask where{dbr:Aliens_Area dbo:lastPublicationD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507546</td>\n",
       "      <td># write a SPARQL query for:\\n# Did bridget jon...</td>\n",
       "      <td>ask where{dbr:Bridget_Jones:_The_Edge_of_Reaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>7260453</td>\n",
       "      <td># write a SPARQL query for:\\n# What is the pop...</td>\n",
       "      <td>select ?x where{dbr:Southern_Yukaghir_language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>6784536</td>\n",
       "      <td># write a SPARQL query for:\\n# Is washington c...</td>\n",
       "      <td>ask where{dbr:Mitchell_County,_North_Carolina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>3733762</td>\n",
       "      <td># write a SPARQL query for:\\n# How many marrie...</td>\n",
       "      <td>select count(*) as ?x where{dbr:Bernardine_Doh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>7637742</td>\n",
       "      <td># write a SPARQL query for:\\n# What are the ba...</td>\n",
       "      <td>select ?x where{dbr:Battle_of_Magdala dbo:resu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>5249851</td>\n",
       "      <td># write a SPARQL query for:\\n# Did the falu re...</td>\n",
       "      <td>ask where{dbr:Falu_red dbo:rgbCoordinateGreen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                             prompt  \\\n",
       "0         3382376  # write a SPARQL query for:\\n# How many ICD9 d...   \n",
       "1         2559914  # write a SPARQL query for:\\n# What was the fa...   \n",
       "2         2990831  # write a SPARQL query for:\\n# Does adventist ...   \n",
       "3         1637967  # write a SPARQL query for:\\n# Did aliens area...   \n",
       "4         1507546  # write a SPARQL query for:\\n# Did bridget jon...   \n",
       "...           ...                                                ...   \n",
       "99995     7260453  # write a SPARQL query for:\\n# What is the pop...   \n",
       "99996     6784536  # write a SPARQL query for:\\n# Is washington c...   \n",
       "99997     3733762  # write a SPARQL query for:\\n# How many marrie...   \n",
       "99998     7637742  # write a SPARQL query for:\\n# What are the ba...   \n",
       "99999     5249851  # write a SPARQL query for:\\n# Did the falu re...   \n",
       "\n",
       "                                              completion  \n",
       "0      select count(*) as ?x where{dbr:Friedreich's_a...  \n",
       "1      select ?x where{dbr:Destrehan_High_School dbo:...  \n",
       "2      ask where{dbr:Adventist_Girls_High_School dbo:...  \n",
       "3      ask where{dbr:Aliens_Area dbo:lastPublicationD...  \n",
       "4      ask where{dbr:Bridget_Jones:_The_Edge_of_Reaso...  \n",
       "...                                                  ...  \n",
       "99995  select ?x where{dbr:Southern_Yukaghir_language...  \n",
       "99996  ask where{dbr:Mitchell_County,_North_Carolina ...  \n",
       "99997  select count(*) as ?x where{dbr:Bernardine_Doh...  \n",
       "99998  select ?x where{dbr:Battle_of_Magdala dbo:resu...  \n",
       "99999  ask where{dbr:Falu_red dbo:rgbCoordinateGreen ...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/nspm-fine-tuning/train.csv\")\n",
    "\n",
    "# df = df[['prompt', 'completion']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b845a409",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "# write a SPARQL query for:\n",
      "# How many leaderFunction did deutsche sammlung von mikroorganismen und zellkulturen have ?\n",
      "\n",
      "SPARQL:\n",
      "select count(*) as ?x where{dbr:Deutsche_Sammlung_von_Mikroorganismen_und_Zellkulturen dbo:leaderFunction ?x }\n",
      "================================================================================\n",
      "Prompt:\n",
      "# write a SPARQL query for:\n",
      "# Who is tuesday's guest's subsidiary ?\n",
      "\n",
      "SPARQL:\n",
      "select ?x where{dbr:Tuesday's_Guest dbo:productionCompany ?x1 . ?x1 dbo:subsidiary ?x }\n",
      "================================================================================\n",
      "Prompt:\n",
      "# write a SPARQL query for:\n",
      "# How many fuel systems did ferrari v8 f1 engine have ?\n",
      "\n",
      "SPARQL:\n",
      "select count(*) as ?x where{dbr:Ferrari_V8_F1_engine dbo:fuelSystem ?x }\n",
      "================================================================================\n",
      "Prompt:\n",
      "# write a SPARQL query for:\n",
      "# Did superior vesical artery branch come from ileal arteries ?\n",
      "\n",
      "SPARQL:\n",
      "ask where{dbr:Ileal_arteries dbo:branchFrom dbr:Superior_vesical_artery }\n",
      "================================================================================\n",
      "Prompt:\n",
      "# write a SPARQL query for:\n",
      "# What colour used in school gann academy ?\n",
      "\n",
      "SPARQL:\n",
      "select ?x where{dbr:Gann_Academy dbo:officialSchoolColour ?x }\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.sample(n=5).iterrows():\n",
    "    print(f\"Prompt:\\n{row['prompt']}\\n\\nSPARQL:\\n{row['completion']}\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80469df3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/mehrzad/.cache/huggingface/datasets/csv/default-a3e3d434e059fc25/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003603219985961914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1074624ec9440448719b6fe16e3d800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006365537643432617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a4a07abc194ac4be977c868eb944f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00583338737487793,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1c909a7414cb5b977c9a2ee3eb84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0024378299713134766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "Generating val split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bb8f4b002b4ec2a0339b36ffc9a489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/mehrzad/.cache/huggingface/datasets/csv/default-a3e3d434e059fc25/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002500295639038086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233bf6e1ce8d47bda4e10499a980f243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'prompt', 'completion'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['Unnamed: 0', 'prompt', 'completion'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## Load Local CSV Filex to Dataset Format using HugginFace datasets library \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    'train': '/home/mehrzad/repos/mehrz/dbpedia/data/nspm-fine-tuning/train.csv', \n",
    "    'val': '/home/mehrzad/repos/mehrz/dbpedia/data/nspm-fine-tuning/val.csv'\n",
    "})\n",
    "\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33833b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# write a SPARQL query for:\n",
      "# How many ICD9 did friedreich's ataxia have ?\n",
      "select count(*) as ?x where{dbr:Friedreich's_ataxia dbo:icd9 ?x }\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['prompt'])\n",
    "print(dataset['train'][0]['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfde84a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# write a SPARQL query for:\n",
      "# Did 1946 vancouver island earthquake lose any of its comrades ?\n",
      "ask where{dbr:1946_Vancouver_Island_earthquake dbo:casualties ?x }\n"
     ]
    }
   ],
   "source": [
    "print(dataset['val'][0]['prompt'])\n",
    "print(dataset['val'][0]['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7570214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, logging, set_seed\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcd02991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=False,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        input_column_name=\"prompt\",\n",
    "        output_column_name=\"completion\"\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.input_column_name = input_column_name\n",
    "        self.output_column_name = output_column_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(prepare_sample_text(next(iterator), self.input_column_name, self.output_column_name))\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    self.current_size += 1\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.LongTensor(input_ids),\n",
    "                        \"labels\": torch.LongTensor(input_ids),\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "def prepare_sample_text(example, input_column_name=\"prompt\", output_column_name=\"completion\"):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"Question: {example[input_column_name]}\\n\\nAnswer: {example[output_column_name]}\"\n",
    "    return text                    \n",
    "                    \n",
    "\n",
    "    \n",
    "                    \n",
    "def chars_token_ratio(dataset, tokenizer, input_column_name=\"prompt\", output_column_name=\"completion\", nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = prepare_sample_text(example, input_column_name, output_column_name)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens                    \n",
    " \n",
    "    \n",
    "    \n",
    "                    \n",
    "def create_datasets(tokenizer):\n",
    "    dataset = load_dataset(\n",
    "        'csv', data_files={\n",
    "            'train': 'data/nspm_100k_train.csv',\n",
    "            'test': 'data/nspm_100k_test.csv'\n",
    "}\n",
    "#         args.dataset_name,\n",
    "#         data_dir=args.subset,\n",
    "#         split=args.split,\n",
    "#         use_auth_token=True,\n",
    "#         num_proc=None, ##\n",
    "#         streaming=args.streaming,\n",
    "    )\n",
    "#     if args.streaming:\n",
    "#         print(\"Loading the dataset in streaming mode\")\n",
    "#         valid_data = dataset.take(args.size_valid_set)\n",
    "#         train_data = dataset.skip(args.size_valid_set)\n",
    "#         train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n",
    "#     else:\n",
    "    train_data = dataset[\"train\"]\n",
    "    valid_data = dataset[\"test\"]\n",
    "    print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
    "\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer, \"prompt\", \"completion\")\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        infinite=True,\n",
    "        seq_length=2048, ##\n",
    "        chars_per_token=chars_per_token,\n",
    "        input_column_name=\"prompt\", ##\n",
    "        output_column_name=\"completion\" ##\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        infinite=False,\n",
    "        seq_length=2048, ##\n",
    "        chars_per_token=chars_per_token,\n",
    "        input_column_name=\"prompt\", ##\n",
    "        output_column_name=\"completion\" ##\n",
    "    )\n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97fb9bcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/mehrzad/.cache/huggingface/datasets/csv/default-2ecec2e06c3917ec/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0070629119873046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71417c335d57430e9275f466f625de9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003381490707397461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb99ba699f4f4bfba524490e95549a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004108905792236328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228abc05d52f48fd9e5ed6c87f2011f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00252532958984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1549977d25488db67508b9a82f8119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/mehrzad/.cache/huggingface/datasets/csv/default-2ecec2e06c3917ec/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002370119094848633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96db1459f23448d5b26472851ec4d76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 70000. Size of the validation set: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 400/400 [00:00<00:00, 6637.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 2.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = create_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "182ee185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '# write a SPARQL query for:\\n# What is the power output of a mercedes-benz m117 engine machine ?\\n\\n# use following prefixes:\\nPREFIX dbr: <http://dbpedia.org/resource/>\\nPREFIX dbo: <http://dbpedia.org/ontology/>',\n",
       " 'completion': 'select ?x where{dbr:Mercedes-Benz_M117_engine dbo:powerOutput ?x }'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394553c",
   "metadata": {},
   "source": [
    "#\n",
    "## Training for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ce1c6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/mehrzad/anaconda3/envs/tf-2.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Found cached dataset csv (/home/mehrzad/.cache/huggingface/datasets/csv/default-2ecec2e06c3917ec/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1116.25it/s]\n",
      "Size of the train set: 70000. Size of the validation set: 30000\n",
      "100%|███████████████████████████████████████| 400/400 [00:00<00:00, 6106.67it/s]\n",
      "The character to token ratio of the dataset is: 2.98\n",
      "Loading the model\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428\n",
      "Starting main loop\n",
      "Training...\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.3077, 'learning_rate': 5e-06, 'epoch': 0.01}\n",
      "{'eval_loss': 1.1688510179519653, 'eval_runtime': 677.2213, 'eval_samples_per_second': 2.172, 'eval_steps_per_second': 2.172, 'epoch': 0.01}\n",
      "{'loss': 1.0817, 'learning_rate': 4.998741355957963e-06, 'epoch': 0.02}\n",
      "{'eval_loss': 0.9329292178153992, 'eval_runtime': 740.1093, 'eval_samples_per_second': 1.988, 'eval_steps_per_second': 1.988, 'epoch': 0.02}\n",
      "{'loss': 0.9201, 'learning_rate': 4.994966691179712e-06, 'epoch': 0.03}\n",
      "{'eval_loss': 0.8598676919937134, 'eval_runtime': 739.5135, 'eval_samples_per_second': 1.989, 'eval_steps_per_second': 1.989, 'epoch': 0.03}\n",
      "{'loss': 0.8631, 'learning_rate': 4.988679806432712e-06, 'epoch': 0.04}\n",
      "{'eval_loss': 0.8117868304252625, 'eval_runtime': 712.6062, 'eval_samples_per_second': 2.064, 'eval_steps_per_second': 2.064, 'epoch': 0.04}\n",
      "{'loss': 0.823, 'learning_rate': 4.9798870320769884e-06, 'epoch': 0.05}\n",
      "{'eval_loss': 0.7829549908638, 'eval_runtime': 723.451, 'eval_samples_per_second': 2.033, 'eval_steps_per_second': 2.033, 'epoch': 0.05}\n",
      "{'loss': 0.8008, 'learning_rate': 4.968597221690986e-06, 'epoch': 0.06}\n",
      "{'eval_loss': 0.7647002339363098, 'eval_runtime': 674.36, 'eval_samples_per_second': 2.181, 'eval_steps_per_second': 2.181, 'epoch': 0.06}\n",
      "{'loss': 0.7838, 'learning_rate': 4.9548217431567665e-06, 'epoch': 0.07}\n",
      "{'eval_loss': 0.7523472905158997, 'eval_runtime': 674.0422, 'eval_samples_per_second': 2.182, 'eval_steps_per_second': 2.182, 'epoch': 0.07}\n",
      "{'loss': 0.7733, 'learning_rate': 4.938574467213519e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 0.7431109547615051, 'eval_runtime': 673.4572, 'eval_samples_per_second': 2.184, 'eval_steps_per_second': 2.184, 'epoch': 0.08}\n",
      "{'loss': 0.7615, 'learning_rate': 4.919871753490892e-06, 'epoch': 0.09}\n",
      "{'eval_loss': 0.7354047298431396, 'eval_runtime': 672.987, 'eval_samples_per_second': 2.186, 'eval_steps_per_second': 2.186, 'epoch': 0.09}\n",
      "{'loss': 0.7561, 'learning_rate': 4.8987324340362445e-06, 'epoch': 0.1}\n",
      "{'eval_loss': 0.7289052605628967, 'eval_runtime': 673.6346, 'eval_samples_per_second': 2.184, 'eval_steps_per_second': 2.184, 'epoch': 0.1}\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.7494, 'learning_rate': 4.875177794352364e-06, 'epoch': 0.11}\n",
      "{'eval_loss': 0.7230220437049866, 'eval_runtime': 672.8871, 'eval_samples_per_second': 2.186, 'eval_steps_per_second': 2.186, 'epoch': 0.11}\n",
      "{'loss': 0.742, 'learning_rate': 4.849231551964771e-06, 'epoch': 0.12}\n",
      "{'eval_loss': 0.718217670917511, 'eval_runtime': 674.6151, 'eval_samples_per_second': 2.181, 'eval_steps_per_second': 2.181, 'epoch': 0.12}\n",
      "{'loss': 0.7395, 'learning_rate': 4.8209198325401815e-06, 'epoch': 0.13}\n",
      "{'eval_loss': 0.7134671211242676, 'eval_runtime': 673.7699, 'eval_samples_per_second': 2.183, 'eval_steps_per_second': 2.183, 'epoch': 0.13}\n",
      "{'loss': 0.7329, 'learning_rate': 4.790271143580174e-06, 'epoch': 0.14}\n",
      "{'eval_loss': 0.7091519236564636, 'eval_runtime': 672.9621, 'eval_samples_per_second': 2.186, 'eval_steps_per_second': 2.186, 'epoch': 0.14}\n",
      "{'loss': 0.7299, 'learning_rate': 4.757316345716554e-06, 'epoch': 0.15}\n",
      "{'eval_loss': 0.7052786350250244, 'eval_runtime': 673.0659, 'eval_samples_per_second': 2.186, 'eval_steps_per_second': 2.186, 'epoch': 0.15}\n",
      "{'loss': 0.7254, 'learning_rate': 4.7220886216373095e-06, 'epoch': 0.16}\n",
      "{'eval_loss': 0.7016641497612, 'eval_runtime': 676.2062, 'eval_samples_per_second': 2.175, 'eval_steps_per_second': 2.175, 'epoch': 0.16}\n",
      "{'loss': 0.7228, 'learning_rate': 4.684623442674463e-06, 'epoch': 0.17}\n",
      "{'eval_loss': 0.6983733177185059, 'eval_runtime': 672.758, 'eval_samples_per_second': 2.187, 'eval_steps_per_second': 2.187, 'epoch': 0.17}\n",
      "{'loss': 0.7181, 'learning_rate': 4.644958533087443e-06, 'epoch': 0.18}\n",
      "{'eval_loss': 0.6951935291290283, 'eval_runtime': 716.9757, 'eval_samples_per_second': 2.052, 'eval_steps_per_second': 2.052, 'epoch': 0.18}\n",
      "{'loss': 0.7162, 'learning_rate': 4.603133832077953e-06, 'epoch': 0.19}\n",
      "{'eval_loss': 0.6923752427101135, 'eval_runtime': 672.5331, 'eval_samples_per_second': 2.187, 'eval_steps_per_second': 2.187, 'epoch': 0.19}\n",
      "{'loss': 0.7117, 'learning_rate': 4.559191453574582e-06, 'epoch': 0.2}\n",
      "{'eval_loss': 0.6893517374992371, 'eval_runtime': 702.9905, 'eval_samples_per_second': 2.092, 'eval_steps_per_second': 2.092, 'epoch': 0.2}\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "^C\n",
      "CPU times: user 6min 40s, sys: 1min 30s, total: 8min 10s\n",
      "Wall time: 23h 37min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python finetune-starcoder-1b.py\\\n",
    "--model_path=\"bigcode/starcoderbase-1b\"\\\n",
    "--eos_token_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2500f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bigcode/starcoderbase-1b-merged\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python merge_peft_adapters.py\\\n",
    "--base_model_name_or_path=\"bigcode/starcoderbase-1b\"\\\n",
    "--peft_model_path=\"./checkpoints/checkpoint-2000/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f79c8",
   "metadata": {},
   "source": [
    "# \n",
    "## Evaluate fine-tuned model with QALD-9-plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdbcc5",
   "metadata": {},
   "source": [
    "\n",
    "### Load fine-tuned checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae1851d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['optimizer.pt',\n",
       " 'scheduler.pt',\n",
       " 'pytorch_model.bin',\n",
       " 'adapter_model.bin',\n",
       " 'README.md',\n",
       " 'rng_state.pth',\n",
       " 'trainer_state.json',\n",
       " 'training_args.bin',\n",
       " 'adapter_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"./checkpoints/checkpoint-2000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ca2467",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint=\"bigcode/starcoderbase-1b-merged\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, use_auth_token=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d88b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for:\n",
      "# What is the foundation of diocese of birobidzhan?\n",
      "\n",
      "# use following prefixes:\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "Answer: select?x where{dbr:Birobidzhan dbo:foundation?x1.?x1 dbo:dbrId?x }<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for:\n",
    "# What is the foundation of diocese of birobidzhan ?\n",
    "\n",
    "# use following prefixes:\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11afa8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for:\n",
      "# What is the capital of Canada?\n",
      "\n",
      "# use following prefixes:\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "Answer: select?x where{dbr:Canada dbo:capital?x1.?x1 dbo:capital?x }<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for:\n",
    "# What is the capital of Canada ?\n",
    "\n",
    "# use following prefixes:\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5b4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for:\n",
      "# What was the population of Italy in 2000?\n",
      "\n",
      "# use following prefixes:\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "Answer: select?x where{dbr:Italy dbo:population?x1.?x1 dbo:year?x }<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for:\n",
    "# What was the population of Italy in 2000 ?\n",
    "\n",
    "# use following prefixes:\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47279862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for:\n",
      "# What is the population of France?\n",
      "\n",
      "# use following prefixes:\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "Answer: select?x where{dbr:France dbo:population?x }<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "## candidate prompt format\n",
    "\n",
    "prompt = \"\"\"\n",
    "# write a SPARQL query for:\n",
    "# What is the population of France ?\n",
    "\n",
    "# use following prefixes:\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo\n",
    "prompt.stirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f61354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b6e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e915867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'question': [{'language': 'en', 'string': 'List all boardgames by GMT.'},\n",
       "  {'language': 'de', 'string': 'Liste die Brettspiele von GMT auf.'},\n",
       "  {'language': 'de', 'string': 'Zeige mir alle Brettspiele von GMT.'},\n",
       "  {'language': 'ru', 'string': 'Перечислите все игры GMT.'},\n",
       "  {'language': 'lt', 'string': 'Išvardinkite visus stalo žaidimus pagal GMT.'},\n",
       "  {'language': 'uk', 'string': 'Перерахуйте всі ігри GMT.'},\n",
       "  {'language': 'lt', 'string': 'Išvardykite visus GMT žaidimus.'},\n",
       "  {'language': 'fr', 'string': 'Listez tous les jeux de société de GMT.'},\n",
       "  {'language': 'es',\n",
       "   'string': '¿Qué juegos de mesa fueron hechos por GMT?',\n",
       "   'keywords': 'juego de mesa ,  GMT '}],\n",
       " 'query': {'sparql': 'PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?uri WHERE { ?uri dbo:publisher res:GMT_Games }'},\n",
       " 'answers': [{'head': {'link': [], 'vars': ['uri']},\n",
       "   'results': {'bindings': [{'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Chandragupta_(board_game)'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Fields_of_Fire_(game)'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Sword_of_Rome'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Paths_of_Glory_(board_game)'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Commands_&_Colors:_Ancients'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Labyrinth:_The_War_on_Terror,_2001_–_%3F'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': 'http://dbpedia.org/resource/Twilight_Struggle'}},\n",
       "     {'uri': {'type': 'uri',\n",
       "       'value': \"http://dbpedia.org/resource/Washington's_War\"}}]}}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/qald_9_plus_train_dbpedia.json', 'r') as f:\n",
    "    qald_9_plus_data = json.load(f)\n",
    "    \n",
    "qald_9_plus_data['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "434c25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 pairs extracted from train set:  \n",
      "\n",
      "List all boardgames by GMT.\n",
      "\n",
      " PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      " PREFIX res: <http://dbpedia.org/resource/>\n",
      " PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      " SELECT ?uri WHERE { ?uri dbo:publisher res:GMT_Games }\n"
     ]
    }
   ],
   "source": [
    "# extracting pairs of question/sparql    \n",
    "    \n",
    "nl_questions, sparql_queries =[],[]\n",
    "\n",
    "for data in qald_9_plus_data['questions']:\n",
    "    nl_questions.append(\n",
    "        next((item['string'] for item in data['question'] if item['language'] == 'en'), None)\n",
    "    )\n",
    "    sparql_queries.append(data['query']['sparql'])\n",
    "    \n",
    "\n",
    "print(f'{len(nl_questions)} pairs extracted from train set: ', \"\\n\")\n",
    "print(nl_questions[0])\n",
    "print('\\n', sparql_queries[0].replace(\">\", '>\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84d503",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "### Calculate BELUE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e301444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str)\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"/\")\n",
    "    parser.add_argument(\"--metric_name\", type=str, default=\"bleu\")\n",
    "    parser.add_argument(\"--input_column_name\", type=str, default=\"prompt\")\n",
    "    parser.add_argument(\"--output_column_name\", type=str, default=\"completion\")\n",
    "    return parser.parse_args()\n",
    "\n",
    " \n",
    "    \n",
    "def read_dataset(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def generate_SPARQL(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=250,\n",
    "        num_beams=5,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "def calculate_bleu(actual, generated, metric_name):\n",
    "    bleu_metric = load_metric(metric_name)\n",
    "    bleu_score = bleu_metric.compute(predictions=[generated.split()], references=[[actual.split()]])\n",
    "    return bleu_score['bleu']\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = get_args()\n",
    "\n",
    "    data = read_dataset(args.data_path)\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token   # Set pad token to EOS token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path, use_auth_token=True).to(device)\n",
    "\n",
    "    \n",
    "    print(f'\\n\\nTokenizer and model successfully loaded on:\\t{device}')\n",
    "    \n",
    "    print(f'\\nCalculating scores with <{args.metric_name}> as evaluation metric\\n...'+\"\\n\"*2)\n",
    "    \n",
    "    results = []\n",
    "    total_bleu_score = 0\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        prompt, actual_sparql = row[args.input_column_name], row[args.output_column_name]\n",
    "        generated_sparql = generate_SPARQL(model, tokenizer, prompt)\n",
    "\n",
    "        bleu_score = calculate_bleu(actual_sparql, generated_sparql, args.metric_name)\n",
    "\n",
    "        results.append((prompt, actual_sparql, generated_sparql, bleu_score))\n",
    "\n",
    "        total_bleu_score += bleu_score\n",
    "\n",
    "    average_bleu_score = total_bleu_score / len(data)\n",
    "\n",
    "    \n",
    "    print(\"\\n\"+\"*\"*50 +f\"Results for {args.model_name_or_path}:\\n\", results)\n",
    "    print(f\"\\nAverage BLEU score for {args.model_name_or_path}: \", average_bleu_score)\n",
    "\n",
    "\n",
    "    # delete model & tokenizer and clear cache \n",
    "    del model\n",
    "    del tokenizer\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b841791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/nspm_100k_test.csv\")\n",
    "\n",
    "df_sample = df_test.sample(n=500, random_state=0)\n",
    "df_sample.to_csv(\"data/nspm_test_500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0a6a84a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:root:Loading tokenizer and model on cuda:0.\n",
      "\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpq78wczpo\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpq78wczpo/_remote_module_non_scriptable.py\n",
      "\n",
      "Calculating scores with <bleu> as evaluation metric\n",
      "...\n",
      "\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py:47: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(metric_name)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py\", line 111, in <module>\n",
      "    main()            \n",
      "  File \"/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py\", line 87, in main\n",
      "    generated_sparql = generate_SPARQL(model, tokenizer, prompt)\n",
      "  File \"/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py\", line 33, in generate_SPARQL\n",
      "    outputs = model.generate(\n",
      "  File \"/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1611, in generate\n",
      "    return self.beam_search(\n",
      "  File \"/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2962, in beam_search\n",
      "    beam_outputs = beam_scorer.process(\n",
      "  File \"/home/mehrzad/anaconda3/envs/dbpedia/lib/python3.10/site-packages/transformers/generation/beam_search.py\", line 238, in process\n",
      "    if self._done[batch_idx]:\n",
      "KeyboardInterrupt\n",
      "CPU times: user 1.69 s, sys: 272 ms, total: 1.96 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python evaluate_model.py\\\n",
    "--model_name_or_path=\"bigcode/starcoderbase-1b-merged\"\\\n",
    "--data_path=\"./data/nspm_test_500.csv\"\\\n",
    "--metric_name=\"bleu\"\\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566c4f04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer and model on cuda:0.\n",
      "\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpa78o8_5c\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpa78o8_5c/_remote_module_non_scriptable.py\n",
      "\n",
      "Calculating scores with <bleu> as evaluation metric\n",
      "...\n",
      "\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py:52: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(metric_name)\n",
      "\n",
      "********************************************************************************\n",
      "Results written to eval_results.csv\n",
      "\n",
      "Average BLEU score for 500 test samples:  0.01604976731943413\n",
      "CPU times: user 8.28 s, sys: 1.46 s, total: 9.74 s\n",
      "Wall time: 12min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python evaluate_model.py\\\n",
    "--model_name_or_path=\"bigcode/starcoderbase-1b-merged\"\\\n",
    "--data_path=\"./data/nspm_test_500.csv\"\\\n",
    "--metric_name=\"bleu\"\\\n",
    "--report_file_name=\"eval_results_02\"\\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029c19d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer and model on cuda:0.\n",
      "\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp__s_mmtm\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp__s_mmtm/_remote_module_non_scriptable.py\n",
      "\n",
      "Calculating scores with <bleu> as evaluation metric\n",
      "...\n",
      "\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/mehrzad/repos/mehrz/dbpedia/neural-qa-mehrzadshm/gsoc/mehrzad/evaluate_model.py:52: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(metric_name)\n",
      "\n",
      "********************************************************************************\n",
      "Results written to eval_results.csv\n",
      "\n",
      "Average BLEU score for 500 test samples:  0.0\n",
      "CPU times: user 19.3 s, sys: 2.81 s, total: 22.1 s\n",
      "Wall time: 39min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python evaluate_model.py\\\n",
    "--model_name_or_path=\"bigcode/starcoderbase-1b\"\\\n",
    "--data_path=\"./data/nspm_test_500.csv\"\\\n",
    "--metric_name=\"bleu\"\\\n",
    "--report_file_name=\"eval_results_starcoder_base_1b\"\\\n",
    "--verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4a5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPARQLPostProcessor:\n",
    "    \n",
    "    ## TODO\n",
    "\n",
    "    # deal with spaces\n",
    "\n",
    "\n",
    "    # for now, remove things based on \"Answer:\" token\n",
    "    def remove_from_token(self, generated_output, reference_token=\"Answer:\"):\n",
    "        return generated_output.replace(reference_token, '').strip()\n",
    "\n",
    "    \n",
    "    def post_process(self, query):\n",
    "        query = self.remove_from_token(query)\n",
    "        #\n",
    "        #\n",
    "        return query\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_SPARQL(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=250,\n",
    "        num_beams=5,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=True\n",
    "    ) \n",
    "    \n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token   # Set pad token to EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bca00da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# write a SPARQL query for;\n",
      "# what is the population of Italy?\n",
      "# use following prefixes:\n",
      "\n",
      "PREFIX dbr: <http://dbpedia.org/resource/>\n",
      "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
      "\n",
      "Answer: select?x where{dbr:Italy dbo:population?x }<|endoftext|>\n",
      "\n",
      "After\n",
      "select?x where{dbr:Italy dbo:population?x }<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "# write a SPARQL query for;\n",
    "# what is the population of Italy?\n",
    "# use following prefixes:\n",
    "\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "post_processor = SPARQLPostProcessor()\n",
    "\n",
    "generated_sparql = generate_SPARQL(model, tokenizer, prompt)\n",
    "print(generated_sparql)\n",
    "\n",
    "print(\"\\nAfter\")\n",
    "\n",
    "clean_generated_sparql = post_processor.post_process(generated_sparql[len(prompt):])\n",
    "print(clean_generated_sparql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6eba957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# write a SPARQL query for;\\n# what is the population of Italy?\\n# use following prefixes:\\n\\nPREFIX dbr: <http://dbpedia.org/resource/>\\nPREFIX dbo: <http://dbpedia.org/ontology/>\\n\\nAnswer: select?x where{dbr:Italy dbo:population?x }<|endoftext|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(generated_sparql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4e3171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: select?x where{dbr:Italy dbo:population?x }<|endoftext|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sparql[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a42ee241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# write a SPARQL query for;\\n# what is the population of Italy?\\n# use following prefixes:\\n\\nPREFIX dbr: <http://dbpedia.org/resource/>\\nPREFIX dbo: <http://dbpedia.org/ontology/>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbpedia-env",
   "language": "python",
   "name": "dbpedia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
